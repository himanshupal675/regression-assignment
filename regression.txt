Answer1 :------->>>>

Simple Linear Regression---->>
                          Simple Linear Regression is a type of Regression algorithms that models the relationship between a dependent variable and a single independent variable. The relationship shown by a Simple Linear Regression model is linear or a sloped straight line, hence it is called Simple Linear Regression.

The key point in Simple Linear Regression is that the dependent variable must be a continuous/real value. However, the independent variable can be measured on continuous or categorical values.

Simple Linear regression algorithm has mainly two objectives:

Model the relationship between the two variables. Such as the relationship between Income and expenditure, experience and Salary, etc.
Forecasting new observations. Such as Weather forecasting according to temperature, Revenue of a company according to the investments in a year, etc.

Multiple linear regression ---->>
                          Multiple Linear Regression is one of the important regression algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable.
                          
                          In Multiple Linear Regression, the target variable(Y) is a linear combination of multiple predictor variables x1, x2, x3, ...,xn. Since it is an enhancement of Simple Linear Regression, so the same is applied for the multiple linear regression equation, the equation becomes:
                          Y= Output/Response variable

b0, b1, b2, b3 , bn....= Coefficients of the model.

x1, x2, x3, x4,...= Various Independent/feature variable.

Answer 2:------->>>>>>>

Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

The linear regression model provides a sloped straight line representing the relationship between the variables. Consider the below image:

Linear Regression in Machine Learning
Mathematically, we can represent a linear regression as:
y= a0+a1x+ ε
Here,

Y= Dependent Variable (Target Variable)
X= Independent Variable (predictor Variable)
a0= intercept of the line (Gives an additional degree of freedom)
a1 = Linear regression coefficient (scale factor to each input value).
ε = random error

The values for x and y variables are training datasets for Linear Regression model representation.

Answer 3: ------>>>>>>>

The easiest way to understand and interpret slope and intercept in linear models is to first understand the slope-intercept formula: y = mx + b. M is the slope or the consistent change between x and y, and b is the y-intercept. Often, the y-intercept represents the starting point of the equation.
 
 
Answer 4:-------->>>>
  
Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.

Answer 5:------>>>>>

Multiple linear regression refers to a statistical technique that uses two or more independent variables to predict the outcome of a dependent variable. The technique enables analysts to determine the variation of the model and the relative contribution of each independent variable in the total variance.

Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables. Whereas linear regress only has one independent variable impacting the slope of the relationship, multiple regression incorporates multiple independent variables.


Answer 6:------>>>>

Multicollinearity is a statistical phenomenon that occurs when two or more independent variables used in a regression analysis highly or strongly correlate. This technique is used in observational studies rather than experimental ones, given its influence on the overall regression model. It finds significance in stock market investment, data science, business analytics program, etc.

Multicollinearity occurs when two or more independent variables have a high correlation with one another in a regression model, which makes it difficult to determine the individual effect of each independent variable on the dependent variable.


Answer 7:------>>>>


Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. In other words, instead of a straight line, the regression line is a curve that fits the data better.

Polynomial regression is a generalization of linear regression, which assumes a linear relationship between the independent and dependent variables. In polynomial regression, the relationship between the variables is not limited to a straight line, but can take on a more complex shape.

The polynomial regression model can be written as:

y = b0 + b1x + b2x^2 + ... + bn*x^n

where y is the dependent variable, x is the independent variable, and b0, b1, b2,..., bn are the coefficients of the polynomial terms. The degree of the polynomial, n, determines the complexity of the model.

One of the key differences between linear and polynomial regression is that linear regression produces a straight line, while polynomial regression can produce a curve. Linear regression is appropriate when there is a linear relationship between the independent and dependent variables, while polynomial regression is appropriate when the relationship is non-linear.

Another difference is that linear regression has only two coefficients (slope and intercept), while polynomial regression has multiple coefficients, one for each term in the polynomial equation.

In summary, polynomial regression is a more flexible and complex model than linear regression, as it allows for a wider range of possible relationships between the independent and dependent variables.

Answer8:------>>>>>

Advantages of Polynomial Regression compared to Linear Regression:
1. Polynomial regression can fit non-linear relationships between the independent and dependent variables, whereas linear regression can only fit linear relationships.
2. Polynomial regression can produce a more accurate prediction of the dependent variable when the relationship between the variables is non-linear.
3. Polynomial regression can model complex relationships between the variables that cannot be captured by linear regression.

Disadvantages of Polynomial Regression compared to Linear Regression:
1. Polynomial regression can lead to overfitting when the degree of the polynomial is too high. This means that the model may perform well on the training data but poorly on the test data.
2. The interpretation of the model can be more complex than linear regression due to the presence of multiple coefficients.

In situations where the relationship between the independent and dependent variables is non-linear, polynomial regression may be preferred over linear regression. For example, if there is a curvilinear relationship between two variables, such as in the case of an inverted U-shaped or a J-shaped curve, polynomial regression may be more appropriate than linear regression. Additionally, if a higher accuracy of prediction is desired, and the number of data points is sufficient, polynomial regression may provide better results. However, if the relationship between the variables is linear, or if there is a limited amount of data available, linear regression may be more suitable. In general, the choice between linear and polynomial regression depends on the nature of the data and the research question being addressed.